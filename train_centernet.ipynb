{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from itertools import product as product\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "from utils.dataset import VOCDataset, DatasetTransform, make_datapath_list, Anno_xml2list, od_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## meta settings\n",
    "import pretrainedmodels\n",
    "model_name = 'resnet18' # choose from any resnets\n",
    "DATASET = \"VOC\"\n",
    "retina = False # for trying retinanets\n",
    "fpn = False # try for fpns\n",
    "scale = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make data.Dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainlist:  16551\n",
      "vallist:  4952\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d1759616df77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m## DatasetTransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mtransform_anno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAnno_xml2list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/simple-centernet.pytorch/utils/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mToAbsoluteCoords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# アノテーションデータの規格化を戻す\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mPhotometricDistort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# 画像の色調などをランダムに変化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mExpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# 画像のキャンバスを広げる\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mRandomSampleCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# 画像内の部分をランダムに抜き出す\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mRandomMirror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# 画像を反転させる\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'mean'"
     ]
    }
   ],
   "source": [
    "if not DATASET == \"COCO\":\n",
    "    # load files\n",
    "    # set your VOCdevkit path here.\n",
    "    vocpath = \"../VOCdevkit/VOC2007\"\n",
    "    train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(vocpath)\n",
    "\n",
    "    vocpath = \"../VOCdevkit/VOC2012\"\n",
    "    train_img_list2, train_anno_list2, _, _ = make_datapath_list(vocpath)\n",
    "\n",
    "    train_img_list.extend(train_img_list2)\n",
    "    train_anno_list.extend(train_anno_list2)\n",
    "\n",
    "    print(\"trainlist: \", len(train_img_list))\n",
    "    print(\"vallist: \", len(val_img_list))\n",
    "\n",
    "    # make Dataset\n",
    "    voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat',\n",
    "                   'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "                   'cow', 'diningtable', 'dog', 'horse',\n",
    "                   'motorbike', 'person', 'pottedplant',\n",
    "                   'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "    if scale == 1:\n",
    "        input_size = 300  # set image size\n",
    "    else:\n",
    "        input_size = 512\n",
    "\n",
    "    ## DatasetTransform\n",
    "    transform = DatasetTransform(input_size)\n",
    "    transform_anno = Anno_xml2list(voc_classes)\n",
    "\n",
    "    # Dataloader\n",
    "    train_dataset = VOCDataset(train_img_list, train_anno_list, phase = \"train\", transform=transform, transform_anno = transform_anno)\n",
    "    val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DatasetTransform(\n",
    "        input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
    "\n",
    "else:\n",
    "    from dataset.coco import COCODetection\n",
    "    import torch.utils.data as data\n",
    "    from utils.dataset import VOCDataset, COCODatasetTransform, make_datapath_list, Anno_xml2list, od_collate_fn\n",
    "\n",
    "    if scale == 1:\n",
    "        input_size = 300  # 画像のinputサイズを300×300にする\n",
    "    else:\n",
    "        input_size = 512\n",
    "\n",
    "    ## DatasetTransform\n",
    "    transform = COCODatasetTransform(input_size)\n",
    "    train_dataset = COCODetection(\"../data/coco/\", image_set=\"train2014\", phase=\"train\", transform=transform)\n",
    "    val_dataset = COCODetection(\"../data/coco/\", image_set=\"val2014\", phase=\"val\", transform=transform)\n",
    "\n",
    "batch_size = int(32/scale)\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn, num_workers=8)\n",
    "\n",
    "# 辞書型変数にまとめる\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm operation\n",
    "batch_iterator = iter(dataloaders_dict[\"val\"])  # イタレータに変換\n",
    "images, targets = next(batch_iterator)  # 1番目の要素を取り出す\n",
    "print(images.size())  # torch.Size([4, 3, 300, 300])\n",
    "print(len(targets))\n",
    "print(targets[1].shape)  # ミニバッチのサイズのリスト、各要素は[n, 5]、nは物体数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show image\n",
    "img = images.numpy()[0]\n",
    "plt.imshow(img.transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show targets\n",
    "target = np.asarray(targets[0])\n",
    "center = np.asarray([target[:,2]-target[:,0], target[:,3]-target[:,1], target[:,4]]).T\n",
    "center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = np.uint8(img*255).transpose(1,2,0)\n",
    "print(img2.shape)\n",
    "# plot stuff centers\n",
    "for c in center:\n",
    "    img2 = cv2.circle(img2, (int(c[0]*input_size), int(c[1]*input_size)), 4, (255, 0, 0))\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
    "# as opencv loads in BGR format by default, we want to show it in RGB.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define Centernet model and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class double_conv(nn.Module):\n",
    "    '''(conv => BN => ReLU) * 2'''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "\n",
    "        #  would be a nice idea if the upsampling could be learned too,\n",
    "        #  but my machine do not have enough memory to handle all those weights\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
    "\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "        \n",
    "    def forward(self, x1, x2=None):\n",
    "        x1 = self.up(x1)\n",
    "        if x2 is not None:\n",
    "            x = torch.cat([x2, x1], dim=1)\n",
    "            # input is CHW\n",
    "            diffY = x2.size()[2] - x1.size()[2]\n",
    "            diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "            x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n",
    "                            diffY // 2, diffY - diffY//2))\n",
    "        else:\n",
    "            x = x1\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "# create backbone.\n",
    "basemodel = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')\n",
    "basemodel = nn.Sequential(*list(basemodel.children())[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class centernet(nn.Module):\n",
    "    '''Mixture of previous classes'''\n",
    "    def __init__(self, n_classes):\n",
    "        super(centernet, self).__init__()\n",
    "        self.base_model = basemodel\n",
    "        \n",
    "        if model_name == \"resnet34\" or model_name==\"resnet18\":\n",
    "            num_ch = 512\n",
    "        else:\n",
    "            num_ch = 2048\n",
    "        \n",
    "        self.up1 = up(num_ch, 512)\n",
    "        self.up2 = up(512, 256)\n",
    "        self.up3 = up(256, 256)\n",
    "        # output classification\n",
    "        self.outc = nn.Conv2d(256, n_classes, 1)\n",
    "        # output residue\n",
    "        self.outr = nn.Conv2d(256, n_classes*4, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        x = self.base_model(x)\n",
    "        \n",
    "        # Add positional info        \n",
    "        x = self.up1(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.up3(x)\n",
    "        outc = self.outc(x)\n",
    "        outr = self.outr(x)\n",
    "        return outc, outr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATASET == \"COCO\":\n",
    "    num_class = 21\n",
    "else:\n",
    "    num_class = 81\n",
    "\n",
    "# test if net works\n",
    "net = centernet(num_class)\n",
    "print(torch.rand([1,3,input_size,input_size]).size())\n",
    "outc, outr = net(torch.rand([1,3,input_size,input_size]))\n",
    "print(outc.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the GPU if there is one, otherwise the cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ssd_model import MultiBoxLoss\n",
    "# define loss\n",
    "criterion = MultiBoxLoss(jaccard_thresh=0.5,neg_pos=3, device=device, half=HALF)\n",
    "# optim\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while the original efficientdet uses cosine annealining lr scheduling, we utilize epoch-based lr decreasing for simplicity.\n",
    "def get_current_lr(epoch): \n",
    "    if DATASET == \"COCO\":\n",
    "        reduce = [120, 180]\n",
    "        lr = 1e-3\n",
    "    else:\n",
    "        reduce = [120,180]\n",
    "        lr = 1e-3\n",
    "        \n",
    "    for i,lr_decay_epoch in enumerate(reduce):\n",
    "        if epoch >= lr_decay_epoch:\n",
    "            lr *= 0.1\n",
    "    return lr\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = get_current_lr(epoch)\n",
    "    print(\"lr is:\", lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up train and eval scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train script. nothing special..\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"used device:\", device)\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # イテレーションカウンタをセット\n",
    "    iteration = 1\n",
    "    epoch_train_loss = 0.0  # epochの損失和\n",
    "    epoch_val_loss = 0.0  # epochの損失和\n",
    "    logs = []\n",
    "\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs+1):\n",
    "        \n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        \n",
    "        # 開始時刻を保存\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "\n",
    "        print('-------------')\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-------------')\n",
    "\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "                print('(train)')\n",
    "            else:\n",
    "                if((epoch+1) % 10 == 0):\n",
    "                    net.eval()   # モデルを検証モードに\n",
    "                    print('-------------')\n",
    "                    print('(val)')\n",
    "                else:\n",
    "                    # 検証は10回に1回だけ行う\n",
    "                    continue\n",
    "\n",
    "            # データローダーからminibatchずつ取り出すループ\n",
    "            for images, targets in dataloaders_dict[phase]:\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                images = images.to(device)\n",
    "                targets = [ann.to(device)\n",
    "                           for ann in targets]  # リストの各要素のテンソルをGPUへ\n",
    "                if HALF:\n",
    "                    images = images.half()\n",
    "                    targets = [ann.half() for ann in targets]\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # 順伝搬（forward）計算\n",
    "                    outputs = net(images)\n",
    "                    #print(outputs[0].type())\n",
    "                    # 損失の計算\n",
    "                    loss_l, loss_c = criterion(outputs, targets)\n",
    "                    loss = loss_l + loss_c\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  # 勾配の計算\n",
    "\n",
    "                        # 勾配が大きくなりすぎると計算が不安定になるので、clipで最大でも勾配2.0に留める\n",
    "                        nn.utils.clip_grad_value_(\n",
    "                            net.parameters(), clip_value=2.0)\n",
    "\n",
    "                        optimizer.step()  # パラメータ更新\n",
    "\n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print('Iter {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
    "                                iteration, loss.item(), duration))\n",
    "                            t_iter_start = time.time()\n",
    "                        # filter inf..\n",
    "                        if not loss.item() == float(\"inf\"):\n",
    "                            epoch_train_loss += loss.item()\n",
    "                        iteration += 1\n",
    "\n",
    "                    # 検証時\n",
    "                    else:\n",
    "                        if not loss.item() == float(\"inf\"):\n",
    "                            epoch_val_loss += loss.item()\n",
    "\n",
    "        # epochのphaseごとのlossと正解率\n",
    "        t_epoch_finish = time.time()\n",
    "        print('-------------')\n",
    "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n",
    "            epoch+1, epoch_train_loss, epoch_val_loss))\n",
    "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
    "        t_epoch_start = time.time()\n",
    "\n",
    "        # ログを保存\n",
    "        log_epoch = {'epoch': epoch+1,\n",
    "                     'train_loss': epoch_train_loss, 'val_loss': epoch_val_loss}\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv(\"log/\"+DATASET+\"_\"+backbone+\"_\" + str(300*scale) +\"log_output.csv\")\n",
    "\n",
    "        epoch_train_loss = 0.0  # epochの損失和\n",
    "        epoch_val_loss = 0.0  # epochの損失和\n",
    "\n",
    "        # ネットワークを保存する\n",
    "        if ((epoch+1) % 5 == 0):\n",
    "            if useBiFPN:\n",
    "                word=\"BiFPN\"\n",
    "            else:\n",
    "                word=\"FPN\"\n",
    "            torch.save(net.state_dict(), 'weights/'+DATASET+\"_\"+backbone+\"_\" + str(300*scale) + \"_\" + word + \"_\" + \n",
    "                       str(epoch+1) + '.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if DATASET == \"COCO\":\n",
    "    num_epochs = 200\n",
    "else:\n",
    "    num_epochs = 200\n",
    "    \n",
    "train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
